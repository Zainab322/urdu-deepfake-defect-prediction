# -*- coding: utf-8 -*-
"""Task1_DS-A-04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aMvgMokr67ZI_xIQsw33VQyIE3GV2D7g
"""

!pip install datasets librosa scikit-learn tensorflow huggingface_hub --quiet

from huggingface_hub import login
login()

from datasets import load_dataset

ds = load_dataset("CSALT/deepfake_detection_dataset_urdu", split="train[:100]")
ds = ds.train_test_split(test_size=0.3, seed=42)

train_data = ds['train']
test_data = ds['test']

print(f"Train size: {len(train_data)}, Test size: {len(test_data)}")

import numpy as np
import librosa
from tqdm import tqdm

def extract_mfcc(audio_array, sr=16000, n_mfcc=13):
    mfcc = librosa.feature.mfcc(y=np.array(audio_array), sr=sr, n_mfcc=n_mfcc).T
    if mfcc.shape[0] < 100:
        mfcc = np.pad(mfcc, ((0, 100 - mfcc.shape[0]), (0, 0)))
    else:
        mfcc = mfcc[:100]
    return mfcc.flatten()

print(train_data[0])

def get_label_from_path(path):
    return 1 if "Bonafide" in path else 0

X_train, y_train = [], []
for row in tqdm(train_data):
    X_train.append(extract_mfcc(row['audio']['array']))
    y_train.append(get_label_from_path(row['audio']['path']))

X_test, y_test = [], []
for row in tqdm(test_data):
    X_test.append(extract_mfcc(row['audio']['array']))
    y_test.append(get_label_from_path(row['audio']['path']))

X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

import numpy as np
unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

# again
from datasets import load_dataset
import random

ds_all = load_dataset("CSALT/deepfake_detection_dataset_urdu", split="train", streaming=True)

bonafide = []
deepfake = []

print("üîç Scanning dataset for 50 bonafide and 50 deepfake samples...")

for example in ds_all:
    path = example['audio']['path']
    if "Bonafide" in path and len(bonafide) < 50:
        bonafide.append(example)
    elif "Deepfake" in path and len(deepfake) < 50:
        deepfake.append(example)

    if len(bonafide) == 50 and len(deepfake) == 50:
        break

subset = bonafide + deepfake
random.shuffle(subset)

print(f"‚úÖ Loaded {len(bonafide)} Bonafide and {len(deepfake)} Deepfake samples")

#mfcc extraction
X_data, y_data = [], []

def extract_mfcc(audio_array, sr=16000, n_mfcc=13):
    import librosa
    import numpy as np
    mfcc = librosa.feature.mfcc(y=np.array(audio_array), sr=sr, n_mfcc=n_mfcc).T
    if mfcc.shape[0] < 100:
        mfcc = np.pad(mfcc, ((0, 100 - mfcc.shape[0]), (0, 0)))
    else:
        mfcc = mfcc[:100]
    return mfcc.flatten()

for row in subset:
    X_data.append(extract_mfcc(row['audio']['array']))
    label = 1 if "Bonafide" in row['audio']['path'] else 0
    y_data.append(label)

X_data = np.array(X_data)
y_data = np.array(y_data)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42, stratify=y_data)
print(f"Train size: {len(X_train)}, Test size: {len(X_test)}")

from datasets import load_dataset
import random

ds_all = load_dataset("CSALT/deepfake_detection_dataset_urdu", split="train", streaming=True)

bonafide = []
deepfake = []
max_samples_per_class = 50

print("üîç Scanning for 50 bonafide and 50 deepfake samples...")

for example in ds_all:
    path = example['audio']['path']
    if "Bonafide" in path and len(bonafide) < max_samples_per_class:
        bonafide.append(example)
    elif "Deepfake" in path and len(deepfake) < max_samples_per_class:
        deepfake.append(example)

    if len(bonafide) == max_samples_per_class and len(deepfake) == max_samples_per_class:
        break

print(f"‚úÖ Loaded: {len(bonafide)} Bonafide, {len(deepfake)} Deepfake")

# Combine and shuffle
subset = bonafide + deepfake
random.shuffle(subset)

from datasets import load_dataset
import random

ds_all = load_dataset("CSALT/deepfake_detection_dataset_urdu", split="train", streaming=True)

bonafide = []
deepfake = []

print("üîç Scanning for Bonafide and Deepfake samples (may take a minute)...")

for i, example in enumerate(ds_all):
    path = example['audio']['path']

    # Add bonafide
    if "Bonafide" in path and len(bonafide) < 50:
        bonafide.append(example)

    # Add deepfake
    elif "Deepfake" in path and len(deepfake) < 50:
        deepfake.append(example)

    # Stop if both are complete
    if len(bonafide) == 50 and len(deepfake) == 50:
        break

    # Safety: prevent infinite loop
    if i > 10000:
        break

print(f"‚úÖ Collected: {len(bonafide)} Bonafide, {len(deepfake)} Deepfake")

# Combine and shuffle
subset = bonafide + deepfake
random.shuffle(subset)

X_data, y_data = [], []
for row in subset:
    try:
        mfcc = extract_mfcc(row['audio']['array'])
        label = 1 if "Bonafide" in row['audio']['path'] else 0
        X_data.append(mfcc)
        y_data.append(label)
    except Exception as e:
        print("‚ö†Ô∏è Skipped a row due to error:", e)

X_data = np.array(X_data)
y_data = np.array(y_data)

# Confirm final balance
from collections import Counter
print("‚úÖ Final class balance:", Counter(y_data))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_data, y_data, test_size=0.3, stratify=y_data, random_state=42
)

# Confirm split balance
print("Train split:", Counter(y_train))
print("Test split:", Counter(y_test))

#using sythetic data
import numpy as np
import pandas as pd
import librosa
import os
import random
from sklearn.model_selection import train_test_split

SAMPLE_RATE = 22050
DURATION = 2
SAMPLES_PER_CLIP = SAMPLE_RATE * DURATION
NUM_SAMPLES_PER_CLASS = 100
NUM_MFCC = 13
N_MFCC_TIME_STEPS = 50

def generate_sine_wave(frequency, duration, sample_rate):
    t = np.linspace(0, duration, int(sample_rate * duration), False)
    tone = 0.5 * np.sin(2 * np.pi * frequency * t)
    return tone

def generate_deepfake_like_audio():
    base = generate_sine_wave(300 + random.randint(0, 100), DURATION, SAMPLE_RATE)
    noise = np.random.normal(0, 0.1, base.shape)
    return base + noise

def generate_bonafide_like_audio():
    return generate_sine_wave(200 + random.randint(0, 50), DURATION, SAMPLE_RATE)

def extract_mfcc(audio):
    mfcc = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC)
    if mfcc.shape[1] < N_MFCC_TIME_STEPS:
        pad_width = N_MFCC_TIME_STEPS - mfcc.shape[1]
        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')
    else:
        mfcc = mfcc[:, :N_MFCC_TIME_STEPS]
    return mfcc.flatten()

data = []
labels = []

for _ in range(NUM_SAMPLES_PER_CLASS):
    bonafide = generate_bonafide_like_audio()
    deepfake = generate_deepfake_like_audio()

    mfcc_bona = extract_mfcc(bonafide)
    mfcc_fake = extract_mfcc(deepfake)

    data.append(mfcc_bona)
    labels.append(0)  # Bonafide

    data.append(mfcc_fake)
    labels.append(1)  # Deepfake

X = np.array(data)
y = np.array(labels)

df = pd.DataFrame(X)
df['label'] = y

df.to_csv('synthetic_urdu_deepfake_audio.csv', index=False)

print("Dataset shape:", df.shape)
print("Sample rows:\n", df.head())

pip install scikit-learn matplotlib

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt

# Load the saved synthetic dataset
df = pd.read_csv('synthetic_urdu_deepfake_audio.csv')

print("Dataset Shape:", df.shape)
df.head()

# Separate input features and target labels
X = df.drop('label', axis=1)
y = df['label']

# 70% training, 30% testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
print(f"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}")

# Normalize feature values
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    "SVM": SVC(kernel='linear', probability=True, random_state=42),
    "Logistic Regression": LogisticRegression(random_state=42),
    "Single-Layer Perceptron": MLPClassifier(hidden_layer_sizes=(20,), max_iter=500, random_state=42)
}

def evaluate_model(name, model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else "N/A"

    print(f"\n{name} Evaluation:")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    print(f"AUC-ROC:   {auc}")

# Train and evaluate each model
for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train_scaled, y_train)
    evaluate_model(name, model, X_test_scaled, y_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

y_train_float = y_train.astype('float32')
y_test_float = y_test.astype('float32')

# Define a simple DNN with 2 hidden layers
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Early stopping to prevent overfitting
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(
    X_train_scaled, y_train_float,
    validation_split=0.2,
    epochs=100,
    batch_size=16,
    callbacks=[early_stop],
    verbose=1
)

from sklearn.metrics import classification_report, roc_auc_score

# Predict probabilities and convert to binary
y_pred_prob = model.predict(X_test_scaled).flatten()
y_pred_class = (y_pred_prob >= 0.5).astype(int)

# Evaluate
print("\nDNN Classification Report:")
print(classification_report(y_test, y_pred_class))

# AUC-ROC
auc_score = roc_auc_score(y_test, y_pred_prob)
print(f"DNN AUC-ROC: {auc_score:.4f}")

import matplotlib.pyplot as plt

# Plot training & validation accuracy/loss
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy over Epochs')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss over Epochs')
plt.legend()

plt.tight_layout()
plt.show()

# Save model to file
model.save("deepfake_dnn_model.h5")
print("Model saved as deepfake_dnn_model.h5")

# Collect results in a dictionary
results = {
    "Model": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1-Score": [],
    "AUC-ROC": []
}

def add_results(name, y_true, y_pred, y_prob):
    results["Model"].append(name)
    results["Accuracy"].append(accuracy_score(y_true, y_pred))
    results["Precision"].append(precision_score(y_true, y_pred))
    results["Recall"].append(recall_score(y_true, y_pred))
    results["F1-Score"].append(f1_score(y_true, y_pred))
    auc = roc_auc_score(y_true, y_prob) if y_prob is not None else np.nan
    results["AUC-ROC"].append(auc)

# Classical models
for name, model in models.items():
    y_pred = model.predict(X_test_scaled)
    y_prob = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, "predict_proba") else None
    add_results(name, y_test, y_pred, y_prob)

# DNN
add_results("DNN", y_pred_class, y_test, y_pred_prob)

# Create DataFrame
results_df = pd.DataFrame(results)
print("\nüîç Model Comparison Summary:")
display(results_df)